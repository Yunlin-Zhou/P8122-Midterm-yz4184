---
title: "P8122 Midterm yz4184"
author: "Yunlin Zhou"
date: '2022-10-21'
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(ri)
library(perm)
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

# Question 1

## 1)

*ACE*

$$\ ACE = E[Y_{1}] - E[Y_{0}] = \frac{12}{20} - \frac{5}{20} = \frac{7}{20} = 0.35 $$

The new treatment is better on average since the ACE is larger than 0.


## 2)

$$E[Y|A=1] - E[Y|A=0] = \frac{5}{10} - \frac{4}{10} = \frac{1}{10} = 0.1$$

Difference in observed group means is 0.1.

From the association parameter above, we can conclude that the new treatment is better on average.

## 3) 

*The results from question 1 and 2 are not same.*

* In the scenario in question 1, the assignment mechanism is regular and known and controlled. $E[Y_{a}]$ are population quantities that are computed by taking an average of potential outcome among all individuals is the population.

* In the scenario in question 2, the assignment mechanism was not known or controlled in advance.The result was only defined post selection. $E[Y|A=a]$ is computes by taking on average of observed outcomes on in the subset of the population with A=a.

* The result in question 2 in smaller, which make the effect of the new treatment seem smaller. The possible reason is that those who received the standard treatment might be healthier. And those who received the new treatment might be weaker. From the truth in question 1, we know that the disease would be prevented in both treatments for subject 3 5 11 16 . In the data in question2, we can see that subject 3 5 11 are assigned to the control group. Similarly, many of those whose disease would not be prevented in both treatments are assigned to treatment group. Thus, the result would be lower in real world data in this case.


## 4) 

a) In an observational study, we typically get all data together
(covariates, treatment, outcomes), and the assignment mechanism is not known or controlled. It will typically be the case that individuals select or are selected to take the active treatment based on their underlying health condition. So the data might arise like the one in question 2. 

b) In a randomized controlled trial, the assignment mechanism is regular and  known and controlled. Also, randomization enforces the assumption of unconfoundedness or exchangeability marginally across covariants. So the data might arise more like the result in theory.


## 5)


* There is a significant difference between the result in question 1 and 2. It is possible that the experiment is not a randomized trial. But we do not have enoght evidence for this assumption.

* This study might not be a Bernoulli randomized experiment. Because the treatment group size and the control group size are equally assigned. But the group a size in Bernoulli randomized experiment is likely to be unequally assigned.


## 6)

From the "truth" we know that: for some weak patients, the disease would not be prevented in both treatments; for some strong patients, the disease would be prevented in both treatments. And for the normal patients, the effects would be different due to different treatments. So the health status of patients is a covariate and we will stratify the patients due to their health status. The process is below:

* filter the patients.
  * weak: Y1 = Y0 = 0
  * normal: Y1 =! Y0
  * strong: Y1 = Y0 = 1

* Completely randomize the units with _sample_frac_ function within each block.

* Combine the "real-world" dataset with different health status data.


```{r, create dataset}
individual <- c(1:20)
Y1 <- c(1,1,1,0,1,0,1,1,0,0,1,0,1,0,0,1,1,1,0,1)
Y0 <- c(0,0,1,0,1,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0)

df = cbind(individual,Y1,Y0)%>%
  as.data.frame()%>%
  mutate(status = ifelse(Y1 == 0 & Y0 == 0, "weak",
                         ifelse(Y1 ==1 & Y0 == 1, "strong","normal")))%>%
  mutate(status = as.factor(status))

df_weak = df %>%
  filter(status == "weak")

df_normal = df %>%
  filter(status == "normal")

df_strong = df %>%
  filter(status == "strong")
```


```{r}
set.seed(8122)

norm_y1 <- sample_frac(df_normal,0.5)%>% mutate(Y0 = NA)
norm_y0 <- df_normal %>%
  filter(!(individual %in% (norm_y1$individual))) %>%
  mutate(Y1 = NA)
norm_rw = rbind(norm_y1,norm_y0) %>%
  arrange(individual)
knitr::kable(norm_rw)
```


```{r}
weak_y1 <- sample_frac(df_weak,0.5)%>% mutate(Y0 = NA)
weak_y0 <- df_weak %>%
  filter(!(individual %in% (weak_y1$individual))) %>%
  mutate(Y1 = NA)
weak_rw = rbind(weak_y1,weak_y0)%>% arrange(individual)
knitr::kable(weak_rw)
```


```{r}
strong_y1 <- sample_frac(df_strong,0.5)%>% mutate(Y0 = NA)
strong_y0 <- df_strong %>%
  filter(!(individual %in% (strong_y1$individual))) %>%
  mutate(Y1 = NA)
strong_rw = rbind(strong_y1,strong_y0)%>% arrange(individual)
knitr::kable(strong_rw)
```


*So my final "real-world" data is :*

```{r}
df_rw = rbind(norm_rw,weak_rw, strong_rw) %>%
  arrange(individual)

knitr::kable(df_rw)
```


## 7)

_Sharp null hypothesis is that there is no treatment effect:_

$$H_0: \tau_i = Y_{1i} - Y_{0i} = 0$$ for all $i$


```{r, data preparation}
df_rw_new = df_rw %>%
pivot_longer(cols = c ("Y1","Y0"),names_to = "A",values_to = "Y", values_drop_na =T) %>%
  mutate(A = ifelse(A == "Y1", 1, 0))
Y = df_rw_new$Y
A = df_rw_new$A

T_stat <- mean(Y[A == 1]) - mean(Y[A == 0])
T_stat
```

First, we build a new data set including assignment(A) and outcome(Y). We also calculated the sharp null t test which is `r T_stat`.


```{r}
Abold_1 = chooseMatrix(9, 4)
Abold_1 = t(Abold_1)
ncol(Abold_1)

Abold_2 = chooseMatrix(7, 4)
Abold_2 = t(Abold_2)
ncol(Abold_2)

Abold_3 = chooseMatrix(4, 2)
Abold_3 = t(Abold_3)
ncol(Abold_3)

ncol(Abold_1)*ncol(Abold_2)*ncol(Abold_3)

Abold <- genperms(A, maxiter = 26460)

Abold <- genperms(A)
```


Then, we generate a matrix to show different possible assignment vectors. There are $9\choose4$$7\choose4$$4\choose2$ possibilities for A. 


```{r, plot the randomization distribution}
rdist <- rep(NA, times = ncol(Abold))
for (i in 1:ncol(Abold)) {
  A_tilde <- Abold[, i]
  rdist[i] <- mean(Y[A_tilde == 1]) - mean(Y[A_tilde == 0])
}

pval <- mean(rdist >= T_stat)

quant <- quantile(rdist,probs = 1-pval)
hist(rdist)
abline(v = quant,col="red")
```


Finally, we use the bootstrap to generate the exact randomization distribution for T, under the sharp null hypothesis of no difference. Also, we calculated the p-value, and added the red line in the plot.


* Since the exact p-value is `r pval` > 0.05, we fail to reject the sharp null hypothesis that there is no difference so there is no treatment effect for all individuals in the sample.


## 8)

1. Create a grid of possible sharp null hypotheses.
2. Calculate p-values for each sharp null.
3. For the point estimate: Pick the value that is ”least surprising” under the null.
4. For the confidence interval: Find the range of the values that we would not reject under the null.

```{r}
grid<-seq(-1,1, by=0.01)
p.ci<-rep(NA,length(grid))

rdist_1 <- rep(NA, times = ncol(Abold))
for (i in 1:length(grid)){
for (k in 1:ncol(Abold)) {
  A_tilde <- Abold[, k]
  rdist_1[k] <- mean(Y[A_tilde == 1]) - mean(Y[A_tilde == 0])+grid[i]
}
  p.ci[i]<-mean(rdist_1 >= T_stat)
}

cbind(p.ci,grid)

point_estimate = mean(grid[which(abs(p.ci - 0.5) == min(abs(p.ci - 0.5)))])

ci = range(grid[which(0.05<p.ci & p.ci<0.95)])
```

* The point estimate is `r point_estimate` and the confidence interval is `r ci`.

* Point estimate = `r point_estimate`: For the petients who received the new treatment are more likely to prevent the disease than the standard treatment on average since `r point_estimate` > 0.

* The true ACE would fall in the range `r ci` at 5% significance level.


# 9)

$$\widehat{SACE} = \overline{Y_{1}^{obs}} - \overline{Y_{0}^{obs}} =  \cfrac{\sum_{i=1}^{N}A_{i}Y_{1i}}{N_{1}} - \cfrac{\sum_{i=1}^{N}(1-A_{i})Y_{0i}}{N_{0}} = \cfrac{5}{10} - \cfrac{2}{10} = \cfrac{3}{10} = 0.3$$


```{r}
point_estimate_neyman = sum(Y[A==1])/10 - sum(Y[A==0])/10
```

* Point estimate = `r point_estimate_neyman`: For the petients who received the new treatment are more likely to prevent the disease than the standard treatment on average since `r point_estimate_neyman` > 0.


$$\widehat{var}(\widehat{SACE}) = \cfrac{S_{1}^{2}}{N_{1}} + \cfrac{S_{0}^{2}}{N_{0}} = 0.0456$$


```{r}
t_crit = qt(0.975, 9)
var = var(Y[A==1])/10 + var(Y[A==0])/10
```


$$CI(low) = \widehat{SACE} - z^*\sqrt{\widehat{var}(\widehat{SACE})} = -0.1828291$$


$$CI(up) = \widehat{SACE} + z^*\sqrt{\widehat{var}(\widehat{SACE})} = 0.7828291$$


```{r}
CI_low = point_estimate_neyman - t_crit*sqrt(var)
CI_up = point_estimate_neyman + t_crit*sqrt(var)

```

* The true ACE would fall in the range between `r CI_low` and  `r CI_up`at 5% significance level.


## 10)

The point estimates from question 8 and 9 very close to the ACE calculated from the "truth". The true ACE is in the range of the confidence interval.

* In Fisher's approach, we compare any test statistic to empirical randomization distribution under sharp null hypothesis. This is a  design-based, assumption-free inference. And we derived a relatively accurate estimation using this approach. 

* In Neyman's approach, we  compare t-statistic to normal or t distribution under average null hypothesis. This approach considers random assignment and random sampling. However, this approach also relies on large N. Since we have a relatively small N, the estimation using this approach might have limitations.














